\documentclass{article}\usepackage{graphicx, color}
\usepackage{eulervm}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\definecolor{fgcolor}{rgb}{0.2, 0.2, 0.2}
\newcommand{\hlnumber}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlfunctioncall}[1]{\textcolor[rgb]{0.501960784313725,0,0.329411764705882}{\textbf{#1}}}%
\newcommand{\hlstring}[1]{\textcolor[rgb]{0.6,0.6,1}{#1}}%
\newcommand{\hlkeyword}[1]{\textcolor[rgb]{0,0,0}{\textbf{#1}}}%
\newcommand{\hlargument}[1]{\textcolor[rgb]{0.690196078431373,0.250980392156863,0.0196078431372549}{#1}}%
\newcommand{\hlcomment}[1]{\textcolor[rgb]{0.180392156862745,0.6,0.341176470588235}{#1}}%
\newcommand{\hlroxygencomment}[1]{\textcolor[rgb]{0.43921568627451,0.47843137254902,0.701960784313725}{#1}}%
\newcommand{\hlformalargs}[1]{\textcolor[rgb]{0.690196078431373,0.250980392156863,0.0196078431372549}{#1}}%
\newcommand{\hleqformalargs}[1]{\textcolor[rgb]{0.690196078431373,0.250980392156863,0.0196078431372549}{#1}}%
\newcommand{\hlassignement}[1]{\textcolor[rgb]{0,0,0}{\textbf{#1}}}%
\newcommand{\hlpackage}[1]{\textcolor[rgb]{0.588235294117647,0.709803921568627,0.145098039215686}{#1}}%
\newcommand{\hlslot}[1]{\textit{#1}}%
\newcommand{\hlsymbol}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlprompt}[1]{\textcolor[rgb]{0.2,0.2,0.2}{#1}}%

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}

\usepackage{alltt}
% ------------------------------------------------------------------------
% Packages
% ------------------------------------------------------------------------
\usepackage{amsmath,amssymb,amsfonts,graphicx,mathtools,setspace}
\usepackage[body={7in, 9in},left=1in,right=1in]{geometry}
\usepackage{url}
\usepackage[colorlinks]{hyperref}

% ------------------------------------------------------------------------
% Macros
% ------------------------------------------------------------------------
%~~~~~~~~~~~~~~~
% List shorthand
%~~~~~~~~~~~~~~~
\newcommand{\BIT}{\begin{itemize}}
\newcommand{\EIT}{\end{itemize}}
\newcommand{\BNUM}{\begin{enumerate}}
\newcommand{\ENUM}{\end{enumerate}}
%~~~~~~~~~~~~~~~
% Text with quads around it
%~~~~~~~~~~~~~~~
\newcommand{\qtext}[1]{\quad\text{#1}\quad}
%~~~~~~~~~~~~~~~
% Shorthand for math formatting
%~~~~~~~~~~~~~~~
\newcommand\mbb[1]{\mathbb{#1}}
\newcommand\mbf[1]{\mathbf{#1}}
\def\mc#1{\mathcal{#1}}
\def\mrm#1{\mathrm{#1}}
%~~~~~~~~~~~~~~~
% Common sets
%~~~~~~~~~~~~~~~
\def\reals{\mathbb{R}} % Real number symbol
\def\integers{\mathbb{Z}} % Integer symbol
\def\rationals{\mathbb{Q}} % Rational numbers
\def\naturals{\mathbb{N}} % Natural numbers
\def\complex{\mathbb{C}} % Complex numbers
%~~~~~~~~~~~~~~~
% Common functions
%~~~~~~~~~~~~~~~
\renewcommand{\exp}[1]{\operatorname{exp}\left(#1\right)} % Exponential
\def\indic#1{\mbb{I}\left({#1}\right)} % Indicator function
\providecommand{\argmax}{\mathop\mathrm{arg max}} % Defining math symbols
\providecommand{\argmin}{\mathop\mathrm{arg min}}
\providecommand{\arccos}{\mathop\mathrm{arccos}}
\providecommand{\dom}{\mathop\mathrm{dom}} % Domain
\providecommand{\range}{\mathop\mathrm{range}} % Range
\providecommand{\diag}{\mathop\mathrm{diag}}
\providecommand{\tr}{\mathop\mathrm{tr}}
\providecommand{\abs}{\mathop\mathrm{abs}}
\providecommand{\card}{\mathop\mathrm{card}}
\providecommand{\sign}{\mathop\mathrm{sign}}
\def\rank#1{\mathrm{rank}({#1})}
\def\supp#1{\mathrm{supp}({#1})}
%~~~~~~~~~~~~~~~
% Common probability symbols
%~~~~~~~~~~~~~~~
\def\E{\mathbb{E}} % Expectation symbol
\def\Earg#1{\E\left[{#1}\right]}
\def\Esubarg#1#2{\E_{#1}\left[{#2}\right]}
\def\P{\mathbb{P}} % Probability symbol
\def\Parg#1{\P\left({#1}\right)}
\def\Psubarg#1#2{\P_{#1}\left[{#2}\right]}
\def\Cov{\mrm{Cov}} % Covariance symbol
\def\Covarg#1{\Cov\left[{#1}\right]}
\def\Covsubarg#1#2{\Cov_{#1}\left[{#2}\right]}
\def\Var{\mrm{Var}}
\def\Vararg#1{\Var\left(#1\right)}
\def\Varsubarg#1#2{\Var_{#1}\left(#2\right)}
\newcommand{\family}{\mathcal{P}} % probability family
\newcommand{\eps}{\epsilon}
\def\absarg#1{\left|#1\right|}
\def\msarg#1{\left(#1\right)^{2}}
\def\logarg#1{\log\left(#1\right)}
%~~~~~~~~~~~~~~~
% Distributions
%~~~~~~~~~~~~~~~
\def\Gsn{\mathcal{N}}
\def\Ber{\textnormal{Ber}}
\def\Bin{\textnormal{Bin}}
\def\Unif{\textnormal{Unif}}
\def\Mult{\textnormal{Mult}}
\def\NegMult{\textnormal{NegMult}}
\def\Dir{\textnormal{Dir}}
\def\Bet{\textnormal{Beta}}
\def\Poi{\textnormal{Poi}}
\def\HypGeo{\textnormal{HypGeo}}
\def\GEM{\textnormal{GEM}}
\def\BP{\textnormal{BP}}
\def\DP{\textnormal{DP}}
\def\BeP{\textnormal{BeP}}
%~~~~~~~~~~~~~~~
% Theorem-like environments
%~~~~~~~~~~~~~~~
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

%-----------------------
% Probability sets
%-----------------------
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
%-----------------------
% vector notation
%-----------------------
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bt}{\mathbf{t}}
\newcommand{\xbar}{\overline{x}}
\newcommand{\Xbar}{\overline{X}}
\newcommand{\tolaw}{\xrightarrow{\mathcal{L}}}
\newcommand{\toprob}{\xrightarrow{\mathbb{P}}}
\newcommand{\laweq}{\overset{\mathcal{L}}{=}}
\newcommand{\F}{\mathcal{F}}
%-----------------------
% Math environments
%-----------------------
%\newcommand{\ba}{\begin{align}}
%\newcommand{\ea}{\end{align}}
%\newcommand{\ba}{\begin{align}}

<<echo = FALSE, message = FALSE, warning = FALSE>>=
library("knitr")
library("ggplot2")
opts_chunk$set(cache = TRUE, fig.width = 5, fig.height = 4,
               fig.align = "center", message = F, warning = F, echo = F)
scale_colour_discrete <- function(...)
  scale_colour_brewer(..., palette="Set2")
theme_set(theme_bw())
silence <- sapply(list.files("../src", full.names = TRUE), read_chunk)
@

\linespread{1.5}

\title{Case Studies in Multitable Data Analysis: The SFHIP Problem}
\author{Kris Sankaran}

\begin{document}
\maketitle

\section{Introduction}

We discuss one approach to a health policy data analysis problem based
on a multitable perspective. Our goals is to illustrate the kind of
situation in which multitable methods can be useful, and also to to
explore whether there is additional structure not considered by
standard methods, which may lead to methods with more interesting
interpretations.

This problem was the focus of a DataKind DataDive in spring 2015. All
code from that event is
\href{https://github.com/DataKind-SF/datadive_201503_sf-health-improvement-partnership}{available
  online}. Some initial dimension reduction code still lingers in that
repository, but this report goes into much more depth.

\subsection{Goals}

The question posed by the San Francisco Health Improvement Partenrship
(SFHIP) was, from a high level, whether it was possible to quantify
the relationship between crime and alcohol use in San Francisco. In
particular, qualitative experience suggests that there are ``bad''
types of liquor stores, that have a negative influence on their
surrounding neighborhoods, in the sense that they lead to less safe,
lower quality of life conditions. As SFHIP has been tasked broadly
with maintaining the health of the populace, this is of basic
interest.

Prior to the event, a few volunteers collected data from sources that
seemed potentially relevant. This included census tract information,
liquor licence data from the city, and informal 311 reports from a
the SF crimespotting database. Indeed, none of the data was collected
by SFHIP -- they simply aggregated it from various public
sources. Indeed, we imagine that, as more data becomes publicly
accessible, the practice of assembling multiple tables to answer
questions of interests will become more common, motivating more
careful study of multitable methods.

\subsection{The Data}

We will focus on three data sets,
\begin{itemize}
  \item \texttt{license}: We were originally given the geographic
    location of each registered alcohol vendor in San Francisco, along
    with the type of license(s) they owned\footnote{This
      \href{https://www.abc.ca.gov/permits/licensetypes.html}{site}
      has more detailed descriptions.} We first filtered away license
    types that were very rare, and simplified further by getting rates
    for each license (there are 23 total) in each census tract, by
    normalizing for population size.
  \item \texttt{census}: This is standard census information from the
    American Community Survey, at the census tract level -- we have
    the population in 2010, a breakdown by demographics, median
    income, and unemployment rates. We have data on 195 tracts (we
    discarded a few with very low population).
  \item \texttt{crime}: These are crime rates (normalized by
    population) for 39 different offenses, within each census
    tract. Again, we have made many simplifications: the raw data
    included the lat / lon and second-by-second resolution time-stamp
    of each 311 report from the crime-spotting database. Further, we
    have discarded less common offenses. Nonetheless, this simplified
    rate data will be sufficient for our present analysis.
\end{itemize}

Essentially, we have reduced all the raw data to a few tables of
census-tract level rates. Yes, we have rarefied all the tracts, and
are losing the fact that some of the rates are much better estimated
than others. We'll leave it as a future exercise to repeat this
analysis without simplifying to rates.

We globally rescaled the demographic features so that each of their
variances equaled the variance across all other features, which were
already approximately on the same scale. Further, for all the methods
except DPCoA, we
\begin{itemize}
\item cube root the rates, so that they look more gaussian, and
\item center the features, so that the usual ``axis of an ellipse''
  interpretation of PCA will apply.
\end{itemize}
We cannot do this same preprocessing for DPCoA, since the function
expects nonnegative data\footnote{The typical ecological application is a
species by sites counts matrix}.

\section{Analysis}

We present dimension reduction techniques in increasing order of
complexity -- we start with single tables (PCA), then pairs (CCA), and
then all three at once (MFA). Then, we consider incorporating spatial
information, via DPCoA.

\subsection{PCA}

We can ignore the multitable aspect of the data -- just concatenate
all the columns and run PCA. This has the downside that the crime
table will have unduly influence, since it has the most
features. Also, we will not have a sense how the groups of variables
are related to each other.

Nonetheless, it is a good baseline, which will serve as a reference
for subsequent methods. See Figures \ref{fig:pca_scores},
\ref{fig:pca_loadings}, and \ref{fig:pca_map} for the results.

<<libraries>>=
@
<<utils>>=
@
<<get-data>>=
@
<<preprocessing>>=
@

<<run-pca>>=
@

\begin{figure}
<<pca-scores>>=
@
\label{fig:pca_scores}
\caption{Each point represents a single tract. The directions can be
  interpreted by referring to Figure \ref{fig:pca_loadings}.}
\end{figure}

\begin{figure}
<<pca-loadings>>=
@
\label{fig:pca_loadings}
\caption{Loadings associated with the scores in Figure
  \ref{fig:pca_scores}.}
\end{figure}

\begin{figure}
<<pca-map-comp>>=
@
\label{fig:pca_map}
\caption{This shades each census tract in San Francisco by the first
  few PCA components.}
\end{figure}

\subsection{CCA}

A first step towards understanding the covariation across tables is to
apply CCA. This has a few limitations -- like PCA, it implicitly
assumes a gaussian model \cite{bach2005probabilistic}, and can only
work with pairs of tables. This is not too much of a problem here,
since we mainly care about the relationship between alcohol licenses
and crime reports, anyway.

<<run-cca>>=
@

<<cca-get-plots>>=
@

\begin{figure}
<<cca-crime-scores>>=
@
\caption{These are the scores for the census tracts, when using the
  crime data. Compare with the scores from the licesnse data, in
  Figure \ref{fig:cca-license-scores}.}
\label{fig:cca-crime-scores}
\end{figure}

\begin{figure}
<<cca-license-scores>>=
@
\caption{These are the scores for the census tracts, when using the
  license data, instead of the crime data, as in Figure
  \ref{fig:cca-crime-scores}.}
\label{fig:cca-license-scores}
\end{figure}

\begin{figure}
<<cca-top-correlation>>=
@
\caption{To assess the success of the CCA optimization, we can plot
  the correlation between the optimized scores across the two
  tables. The correlation is relatively strong, suggesting that in
  fact the tables are quite related, at a global level.}
\end{figure}

\subsection{MFA}

MFA lets us work with all tables simultaneously. We use the
implementation provided by \cite{le2008factominer}. The results are
quite similar to the original PCA, but we can now get extra
group-level covariation information -- apparently the census and crime
data are more related than either is to alcohol license rates.

The first two dimensions capture about 50\% of the variance. Plots
analogous to those displayed earlier are given in Figures
\ref{fig:mfa-scores}, \ref{fig:mfa-loadings}, \ref{fig:mfa-map}.

<<run-mfa>>=
@

\begin{figure}
<<mfa-scores>>=
@
\caption{Scores computed from MFA.}
\end{figure}

\begin{figure}
<<mfa-loadings>>=
@
\caption{Loadings computed from MFA.}
\end{figure}

\begin{figure}
<<mfa-scores-map>>=
@
\caption{Scores for each census tract, from MFA.}
\end{figure}

\subsection{DPCoA}

So far, none of our analysis has accounted for the (known)
geographical distances between census tracts. For this, we can apply
DPCoA \cite{pavoine2013new}, but we will have to forfeit the
multitable interpretations possible with CCA and MFA. Also, as the
method (or at least, the \texttt{R} function) only works with
nonnegative data, we will have to work with the untransformed,
uncentered version of the data.

Our results are shown in Figures \ref{fig:dpcoa-scores},
\ref{fig:dpcoa-loadings}, \ref{fig:dpcoa-scores-map}. The scores had
already been smooth on the maps from our earlier approaches, even
without enforcing spatial homogeneity, but now even stronger
neighborhood clustering is apparent. Also, regions (in the south and
southwest) that seemed to share essentially the same scores now appear
quite different. It is not obvious to me whether this is due to
differences in methdology or the absense of processing.

<<get-distances>>=
@

<<run-dpcoa>>=
@

\begin{figure}
<<dpcoa-scores-plot>>=
@
\caption{DPCoA scores, using geographical distances between
  tracts. Note that census tracts that are close to each other tend to
  have scores that are close to each other. The effect is almost a
  little concerning -- how much is the actual data (not geographic
  distance) contributing?}
\label{fig:dpcoa-scores}
\end{figure}

\begin{figure}
<<dpcoa-loadings>>=
@
\caption{Loadings associated with the DPCoA.}
\label{fig:dpcoa-loadings}
\end{figure}

\begin{figure}
<<dpcoa-scores-map>>=
@
\caption{DPCoA scores, on a map of San Francisco.}
\label{fig:dpcoa-map}
\end{figure}

\section{Appendix}

\subsection{Histograms}

What was the effect of the cube root transformation? Figures
\ref{fig:hist-untrans} and \ref{fig:hist-trans} give histograms for a
subset of features in the data before and after the transformation.

\begin{figure}
<<untransformed-histograms>>=
@
\caption{Histograms of a few features, before taking cube root
  transformations and centering, but after an overall scaling on the
  demographic features.}
\label{fig:hist-untrans}
\end{figure}

\begin{figure}
<<transformed-histograms>>=
@
\caption{Histograms of the same featuers, after transformations.}
\label{fig:hist-trans}
\end{figure}

\bibliographystyle{unsrt}
\bibliography{multitable_sf_hip}

\end{document}
