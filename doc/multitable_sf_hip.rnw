\documentclass{scrartcl}\usepackage{graphicx, color}
\usepackage{eulervm}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother


\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\definecolor{fgcolor}{rgb}{0.2, 0.2, 0.2}
\newcommand{\hlnumber}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlfunctioncall}[1]{\textcolor[rgb]{0.501960784313725,0,0.329411764705882}{\textbf{#1}}}%
\newcommand{\hlstring}[1]{\textcolor[rgb]{0.6,0.6,1}{#1}}%
\newcommand{\hlkeyword}[1]{\textcolor[rgb]{0,0,0}{\textbf{#1}}}%
\newcommand{\hlargument}[1]{\textcolor[rgb]{0.690196078431373,0.250980392156863,0.0196078431372549}{#1}}%
\newcommand{\hlcomment}[1]{\textcolor[rgb]{0.180392156862745,0.6,0.341176470588235}{#1}}%
\newcommand{\hlroxygencomment}[1]{\textcolor[rgb]{0.43921568627451,0.47843137254902,0.701960784313725}{#1}}%
\newcommand{\hlformalargs}[1]{\textcolor[rgb]{0.690196078431373,0.250980392156863,0.0196078431372549}{#1}}%
\newcommand{\hleqformalargs}[1]{\textcolor[rgb]{0.690196078431373,0.250980392156863,0.0196078431372549}{#1}}%
\newcommand{\hlassignement}[1]{\textcolor[rgb]{0,0,0}{\textbf{#1}}}%
\newcommand{\hlpackage}[1]{\textcolor[rgb]{0.588235294117647,0.709803921568627,0.145098039215686}{#1}}%
\newcommand{\hlslot}[1]{\textit{#1}}%
\newcommand{\hlsymbol}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlprompt}[1]{\textcolor[rgb]{0.2,0.2,0.2}{#1}}%

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}

\usepackage{alltt}
% ------------------------------------------------------------------------
% Packages
% ------------------------------------------------------------------------
\usepackage{amsmath,amssymb,amsfonts,graphicx,mathtools,setspace}
\usepackage[body={7in, 9in},left=1in,right=1in]{geometry}
\usepackage{url}
\usepackage[colorlinks]{hyperref}

% ------------------------------------------------------------------------
% Macros
% ------------------------------------------------------------------------
%~~~~~~~~~~~~~~~
% List shorthand
%~~~~~~~~~~~~~~~
\newcommand{\BIT}{\begin{itemize}}
\newcommand{\EIT}{\end{itemize}}
\newcommand{\BNUM}{\begin{enumerate}}
\newcommand{\ENUM}{\end{enumerate}}
%~~~~~~~~~~~~~~~
% Text with quads around it
%~~~~~~~~~~~~~~~
\newcommand{\qtext}[1]{\quad\text{#1}\quad}
%~~~~~~~~~~~~~~~
% Shorthand for math formatting
%~~~~~~~~~~~~~~~
\newcommand\mbb[1]{\mathbb{#1}}
\newcommand\mbf[1]{\mathbf{#1}}
\def\mc#1{\mathcal{#1}}
\def\mrm#1{\mathrm{#1}}
%~~~~~~~~~~~~~~~
% Common sets
%~~~~~~~~~~~~~~~
\def\reals{\mathbb{R}} % Real number symbol
\def\integers{\mathbb{Z}} % Integer symbol
\def\rationals{\mathbb{Q}} % Rational numbers
\def\naturals{\mathbb{N}} % Natural numbers
\def\complex{\mathbb{C}} % Complex numbers
%~~~~~~~~~~~~~~~
% Common functions
%~~~~~~~~~~~~~~~
\renewcommand{\exp}[1]{\operatorname{exp}\left(#1\right)} % Exponential
\def\indic#1{\mbb{I}\left({#1}\right)} % Indicator function
\providecommand{\argmax}{\mathop\mathrm{arg max}} % Defining math symbols
\providecommand{\argmin}{\mathop\mathrm{arg min}}
\providecommand{\arccos}{\mathop\mathrm{arccos}}
\providecommand{\dom}{\mathop\mathrm{dom}} % Domain
\providecommand{\range}{\mathop\mathrm{range}} % Range
\providecommand{\diag}{\mathop\mathrm{diag}}
\providecommand{\tr}{\mathop\mathrm{tr}}
\providecommand{\abs}{\mathop\mathrm{abs}}
\providecommand{\card}{\mathop\mathrm{card}}
\providecommand{\sign}{\mathop\mathrm{sign}}
\def\rank#1{\mathrm{rank}({#1})}
\def\supp#1{\mathrm{supp}({#1})}
%~~~~~~~~~~~~~~~
% Common probability symbols
%~~~~~~~~~~~~~~~
\def\E{\mathbb{E}} % Expectation symbol
\def\Earg#1{\E\left[{#1}\right]}
\def\Esubarg#1#2{\E_{#1}\left[{#2}\right]}
\def\P{\mathbb{P}} % Probability symbol
\def\Parg#1{\P\left({#1}\right)}
\def\Psubarg#1#2{\P_{#1}\left[{#2}\right]}
\def\Cov{\mrm{Cov}} % Covariance symbol
\def\Covarg#1{\Cov\left[{#1}\right]}
\def\Covsubarg#1#2{\Cov_{#1}\left[{#2}\right]}
\def\Var{\mrm{Var}}
\def\Vararg#1{\Var\left(#1\right)}
\def\Varsubarg#1#2{\Var_{#1}\left(#2\right)}
\newcommand{\family}{\mathcal{P}} % probability family
\newcommand{\eps}{\epsilon}
\def\absarg#1{\left|#1\right|}
\def\msarg#1{\left(#1\right)^{2}}
\def\logarg#1{\log\left(#1\right)}
%~~~~~~~~~~~~~~~
% Distributions
%~~~~~~~~~~~~~~~
\def\Gsn{\mathcal{N}}
\def\Ber{\textnormal{Ber}}
\def\Bin{\textnormal{Bin}}
\def\Unif{\textnormal{Unif}}
\def\Mult{\textnormal{Mult}}
\def\NegMult{\textnormal{NegMult}}
\def\Dir{\textnormal{Dir}}
\def\Bet{\textnormal{Beta}}
\def\Poi{\textnormal{Poi}}
\def\HypGeo{\textnormal{HypGeo}}
\def\GEM{\textnormal{GEM}}
\def\BP{\textnormal{BP}}
\def\DP{\textnormal{DP}}
\def\BeP{\textnormal{BeP}}
%~~~~~~~~~~~~~~~
% Theorem-like environments
%~~~~~~~~~~~~~~~
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

%-----------------------
% Probability sets
%-----------------------
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
%-----------------------
% vector notation
%-----------------------
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bt}{\mathbf{t}}
\newcommand{\xbar}{\overline{x}}
\newcommand{\Xbar}{\overline{X}}
\newcommand{\tolaw}{\xrightarrow{\mathcal{L}}}
\newcommand{\toprob}{\xrightarrow{\mathbb{P}}}
\newcommand{\laweq}{\overset{\mathcal{L}}{=}}
\newcommand{\F}{\mathcal{F}}
\renewenvironment{knitrout}{\begin{singlespace}}{\end{singlespace}}
%-----------------------
% Math environments
%-----------------------
%\newcommand{\ba}{\begin{align}}
%\newcommand{\ea}{\end{align}}
%\newcommand{\ba}{\begin{align}}

<<echo = FALSE, message = FALSE, warning = FALSE>>=
library("knitr")
library("ggplot2")
library("grid")
opts_chunk$set(cache = F, fig.width = 3, fig.height = 3,
               message = F, warning = F, echo = F,
               fig.show = "hold")
scale_colour_discrete <- function(...)
  scale_colour_brewer(..., palette="Set2")
small_theme <- function(base_size = 7, base_family = "Helvetica") {
  theme_bw(base_size = base_size, base_family = base_family) %+replace%
  theme(axis.title = element_text(size = rel(0.8)),
        legend.title = element_text(size = rel(0.8)),
        legend.text = element_text(size = rel(0.8)),
        plot.title = element_text(size = rel(1.1)),
        plot.margin = unit(c(0.1, .1, .1, .1), "lines")
        )
}

theme_set(small_theme())
silence <- sapply(list.files("../src", full.names = TRUE), read_chunk)
@

\linespread{1.5}

\title{Case Studies in Multitable Data Analysis: The SFHIP Problem}
\author{Kris Sankaran}

\begin{document}
\maketitle

\section{Introduction}

We discuss a multitable approach to a health policy data analysis
problem. Our goals are to illustrate the kind of
situation in which multitable methods can be useful and to motivate
new methodological work through real data.

This problem was the focus of a DataKind DataDive in spring 2015. All
code from that event is
\href{https://github.com/DataKind-SF/datadive_201503_sf-health-improvement-partnership}{available
  online}. Some preliminary multitable code still lingers in that
repository, but this report goes into more depth.

\subsection{Goals}

The question posed by the San Francisco Health Improvement Partnership
(SFHIP) was, from a high level, whether it was possible to quantify
the relationship between crime and alcohol use in San Francisco. In
particular, qualitative experience suggests that there are ``bad''
types of liquor stores, which have a negative influence on their
surrounding neighborhoods, by attracting criminal activity. As SFHIP
has been tasked broadly with maintaining the health of the populace --
which includes public safety --  this is of basic interest.

Prior to the event, a few volunteers collected data from sources that
seemed potentially relevant. This included census tract information,
liquor license data from the city, and informal 311 reports from a
the \href{http://sanfrancisco.crimespotting.org/}{SF crimespotting database}. Indeed, none of the data was collected
for this study -- it was simply aggregated from various public
sources. We prophesize that, as more data becomes publicly
accessible, the practice of assembling multiple tables to answer
questions of interest will become more common, motivating deeper study
of multitable methods.

\subsection{The Data}

We will focus on three data sets,
\begin{itemize}
  \item \texttt{license}: We were originally given the geographic
    location of each registered alcohol vendor in San Francisco, along
    with the type of license(s) they owned\footnote{This
      \href{https://www.abc.ca.gov/permits/licensetypes.html}{site}
      has more detailed descriptions.} We first filtered away license
    types that were very rare, and simplified further by getting rates
    for each license (there are 23 total) in each census tract, by
    normalizing by population size.
  \item \texttt{census}: This is standard census information from the
    American Community Survey. We have the population in 2010, a
    breakdown by demographics, median income, and unemployment
    rates for 195 census tracts (we discarded a few with very low
    population).
  \item \texttt{crime}: These are crime rates (normalized by
    population) for 39 different offenses, within each census
    tract. Again, we have made many simplifications: the raw data
    included the latitude / longitude and a second-by-second time-stamp
    of each 311 report from the crime-spotting database. Further, we
    have discarded less common offenses. Nonetheless, this simplified
    rate data will be sufficient for our present analysis.
\end{itemize}

Essentially, we have reduced all the raw data to a few tables of
census-tract level rates. Yes, we are rarefying, and
are losing the fact that some of the rates are much better estimated
than others. We'll leave it as a future exercise to repeat this
analysis without simplifying to rates.

We globally rescaled the demographic features so that each of their
variances equaled the variance across all other features, which were
already approximately on the same scale. Further, for all the methods
except DPCoA, we
\begin{itemize}
\item cube root the rates, so that they look more gaussian, and
\item center the features, so that the usual ``axis of an ellipse''
  interpretation of PCA applies.
\end{itemize}
We cannot do this same preprocessing for DPCoA, since the function
expects nonnegative data\footnote{The typical ecological application is a
species by sites counts matrix}.

\section{Analysis}

We present dimension reduction techniques in increasing order of
complexity -- we start with single tables (PCA), then pairs (CCA), and
then all three at once (MFA). Finally, we consider incorporating spatial
information, via DPCoA.

\subsection{PCA}

We can ignore the multitable aspect of the data -- just concatenate
all the columns and run PCA. This has the downside that the crime
table will have unduly influence (it has the most columns). Also, we
will not have a sense how the groups of variables are related to each
other.

Nonetheless, it is a good baseline, which will serve as a reference
for subsequent methods. See Figures \ref{fig:pca_scores} and
\ref{fig:pca_map} for the results.

<<libraries>>=
@

<<utils>>=
@
<<get-data>>=
@
<<preprocessing>>=
@

<<run-pca>>=
@

\begin{figure}
<<pca-scores>>=
@
\caption{Each on the left point represents a single tract. The directions can be
  interpreted by referring to loadings on the right.}
\label{fig:pca_scores}
\end{figure}

\begin{figure}
<<pca-map-comp, fig.align = "center", fig.height = 4, fig.width = 4>>=
@
\caption{The PCA scores on a map of San Francisco.}
\label{fig:pca_map}
\end{figure}

\subsection{CCA}

A first step towards understanding the covariation across tables is to
apply CCA. This has a few limitations -- like PCA, it implicitly
assumes a gaussian model \cite{bach2005probabilistic}, and can only
work with pairs of tables. This is not too much of a problem here,
since we mainly care about the relationship between alcohol licenses
and crime reports, anyways. See Figures
\ref{fig:cca-crime-license-scores} and \ref{fig:cca-cors} for the
results.

<<run-cca>>=
@

<<cca-get-plots>>=
@

\begin{figure}
<<cca-crime-license-scores>>=
@
\caption{These are the scores for the census tracts, when using the
  crime and license data, respectively.}
\label{fig:cca-crime-license-scores}
\end{figure}

\begin{figure}
<<cca-top-correlation, fig.align = "center">>=
@
\caption{To assess the success of the CCA optimization, we can plot
  the correlation between the optimized scores across the two
  tables. The correlation is relatively strong, suggesting that
  the tables are, in fact, quite related, at a global level.}
\label{fig:cca-cors}
\end{figure}

\subsection{MFA}

MFA lets us work with all tables simultaneously. We use the
implementation provided by \cite{le2008factominer}. The results are
quite similar to the original PCA, but we can now get extra
group-level covariation information -- apparently the census and crime
data are more related than either is to alcohol license rates.

The first two dimensions capture about 50\% of the variation. Plots
analogous to those displayed earlier are given in Figures
\ref{fig:mfa-scores-loadings} and \ref{fig:mfa-map}.

<<run-mfa>>=
@

\begin{figure}
<<mfa-scores-loadings>>=
@
\caption{Scores and loadings computed from MFA.}
\label{fig:mfa-scores-loadings}
\end{figure}

\begin{figure}
<<mfa-scores-map, fig.height = 4, fig.width = 4, fig.align = "center">>=
@
\caption{MFA scores on a map of San Francisco.}
\label{fig:mfa-map}
\end{figure}

\subsection{DPCoA}

So far, none of our analysis has accounted for the (known)
geographical distances between census tracts. For this, we can apply
DPCoA \cite{pavoine2013new}, but we will have to forfeit the
multitable interpretations possible with CCA and MFA. Also, as the
method (or at least, the \texttt{R} function) only works with
nonnegative data, we will have to work with the untransformed,
uncentered version of the data.

Our results are shown in Figures \ref{fig:dpcoa-scores-loadings} and
\ref{fig:dpcoa-scores-map}. The scores had already been smooth on the
maps from our earlier approaches, even without enforcing spatial
homogeneity, but now even stronger neighborhood clustering is
apparent. Also, regions (in the south and southwest) that seemed to
share essentially the same scores now appear quite different. It is
not obvious to me whether this is due to differences in methdology or
the absense of processing.

<<get-distances>>=
@

<<run-dpcoa>>=
@

\begin{figure}
<<dpcoa-scores-loadings>>=
@
\caption{DPCoA scores, using geographical distances between
  tracts. Note that census tracts that are close to each other tend to
  have scores that are close to each other. The effect is almost a
  little concerning -- how much is the actual data (not geographic
  distance) contributing?}
\label{fig:dpcoa-scores-loadings}
\end{figure}

\begin{figure}
<<dpcoa-scores-map, fig.align = "center", fig.height = 4, fig.width = 4>>=
@
\caption{DPCoA scores on a map of San Francisco.}
\label{fig:dpcoa-scores-map}
\end{figure}

\section{Appendix}

\subsection{Histograms}

What was the effect of the cube root transformation? Figure
\ref{fig:hist-trans} gives histograms for a subset of features in the
data before and after the transformation.

\begin{figure}
<<histograms-transformation>>=
@
\caption{Histograms of a few features, before and after taking cube
  root transformations and centering.}
\label{fig:hist-trans}
\end{figure}

\bibliographystyle{unsrt}
\bibliography{multitable_sf_hip}

\end{document}
